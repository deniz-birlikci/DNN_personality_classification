{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN_16Personalities",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Reddit API"
      ],
      "metadata": {
        "id": "6xIgrRUA0cdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary packages (Run only once)\n",
        "!pip install praw"
      ],
      "metadata": {
        "id": "TuNF2UHcNeon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p0R_CPk70B8W"
      },
      "outputs": [],
      "source": [
        "import praw\n",
        "import json\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_client_id = 'my_client_id'\n",
        "my_client_secret = 'my_client_secret'\n",
        "my_user_agent = 'my_user_agent'\n",
        "my_username = 'my_username'\n",
        "my_password = 'my_password'\n",
        "\n",
        "def getMyCredentials():\n",
        "  return (my_client_id, my_client_secret, my_user_agent, my_username, my_password)"
      ],
      "metadata": {
        "id": "5dqZaD35Sfo8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getMyCredentials is a function that I use to fetch my information.\n",
        "# You can hard code variable values if desired\n",
        "my_client_id, my_client_secret, my_user_agent, my_user, my_password = getMyCredentials()\n",
        "\n",
        "# Create the Reddit instance\n",
        "reddit = praw.Reddit(\n",
        "    client_id = my_client_id, \n",
        "    client_secret = my_client_secret, \n",
        "    password = my_password,\n",
        "    user_agent = my_user_agent,\n",
        "    username = my_user,\n",
        "    check_for_async = False # This is to prevent warning messages when running on google collab\n",
        ")\n",
        "reddit.read_only = True"
      ],
      "metadata": {
        "id": "TktdPwS4F02I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A list of all of the subreddits that we want to get posts from\n",
        "subredditList = [\n",
        " 'istj',\n",
        " 'istp',\n",
        " 'isfj',\n",
        " 'isfp',\n",
        " 'infj',\n",
        " 'infp',\n",
        " 'intj',\n",
        " 'intp',\n",
        " 'estp',\n",
        " 'estj',\n",
        " 'esfp',\n",
        " 'esfj',\n",
        " 'enfp',\n",
        " 'enfj',\n",
        " 'entp',\n",
        " 'entj'\n",
        "]"
      ],
      "metadata": {
        "id": "jGDbMibmHP4M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the post titles and post bodies, given a subreddit\n",
        "def getPosts(subredditName, limit = 100000, metric = 'top'):\n",
        "  # Create a subreddit instance (PRAW)\n",
        "  subreddit = reddit.subreddit(subredditName)\n",
        "\n",
        "  # Based on which metric we have requested, PRAW libary pulls  \n",
        "  # limit number of posts from the subreddit based on the metric\n",
        "  if metric == 'hot':\n",
        "    posts = subreddit.hot(time_filter=\"all\", limit = limit)\n",
        "  elif metric == 'new':\n",
        "    posts = subreddit.new(time_filter=\"all\", limit = limit)\n",
        "  else:\n",
        "    posts = subreddit.top(time_filter=\"all\", limit = limit)\n",
        "  \n",
        "  # For all of the posts we have, we check if we have a text body of more than\n",
        "  # 50 characters, if not, we disregard them\n",
        "  filteredPosts = [post for post in posts if len(post.selftext) > 50]\n",
        "\n",
        "  # For those posts with 50 characters, we get their title and body text\n",
        "  filteredEntries = [(post.title + post.selftext) for post in filteredPosts]\n",
        "\n",
        "  # TODO: Also include the comments for the analysis\n",
        "\n",
        "  # Filtered entries list is a list of posts (represented as strings) from the\n",
        "  # given subreddit\n",
        "  return filteredEntries\n",
        "\n",
        "def getPostsFromSubredditList(subredditList, limit = 100000):\n",
        "  dataset = []\n",
        "  labels = []\n",
        "\n",
        "  for subredditIdx in range(16):\n",
        "    time.sleep(30)\n",
        "\n",
        "    subreddit = subredditList[subredditIdx]\n",
        "    print(f\"Getting posts from r/{subreddit}\")\n",
        "\n",
        "    currentEntries = getPosts(subreddit, limit)\n",
        "    dataset = dataset + currentEntries\n",
        "\n",
        "    num_labels = len(currentEntries)\n",
        "    currentLabels = [subredditIdx] * num_labels\n",
        "    labels = labels + currentLabels\n",
        "\n",
        "  return dataset, labels\n"
      ],
      "metadata": {
        "id": "gDy2mMm9Py5t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we don't have the dataset, we can run:"
      ],
      "metadata": {
        "id": "ODW1qXP9GZZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, labels = getPostsFromSubredditList(subredditList, limit = 100000)\n",
        "\n",
        "data = {\n",
        "    'dataset' : dataset,\n",
        "    'labels' : labels\n",
        "}\n",
        "\n",
        "with open('/content/reddit_data.json', 'w') as f:\n",
        "    json.dump(data, f)"
      ],
      "metadata": {
        "id": "dp7UeAkJJ2Ch"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have our dataset already, we can run:"
      ],
      "metadata": {
        "id": "5AAMQd-BGcFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/reddit_data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "dataset, labels = data['dataset'], data['labels']"
      ],
      "metadata": {
        "id": "gFfCGTWKGe73"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-Processing Text"
      ],
      "metadata": {
        "id": "6Q_B_puT0eru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import json\n",
        "from typing import List\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "jLVtPPza1B8O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwordsList = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "personalityList = [\n",
        " 'istj',\n",
        " 'istp',\n",
        " 'isfj',\n",
        " 'isfp',\n",
        " 'infj',\n",
        " 'infp',\n",
        " 'intj',\n",
        " 'intp',\n",
        " 'estp',\n",
        " 'estj',\n",
        " 'esfp',\n",
        " 'esfj',\n",
        " 'enfp',\n",
        " 'enfj',\n",
        " 'entp',\n",
        " 'entj']\n",
        "\n",
        "stopwords = set(stopwordsList)\n",
        "personalities = set(personalityList)\n"
      ],
      "metadata": {
        "id": "UHOAwck20Lu4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def isLegalToken(token : str) -> bool:\n",
        "  flagList = [personality in token.lower() for personality in personalities]\n",
        "  condition = (\n",
        "      token.isalpha()\n",
        "      and token not in stopwords\n",
        "      and (True not in flagList)\n",
        "      and len(token) > 2\n",
        "  )\n",
        "  return condition\n",
        "\n",
        "def clean_post(post : str) -> List[str]:\n",
        "  # Split the text into tokens (by whitespace)\n",
        "  post = post.replace(\".\", \" \")\n",
        "  tokens = post.split()\n",
        "  # Filter out punctuations, numbers, and also make everything lowercase\n",
        "  # (https://www.delftstack.com/howto/python/python-replace-multiple-characters/#use-str.replace-to-replace-multiple-characters-in-python)\n",
        "  transTable = str.maketrans(string.ascii_uppercase, string.ascii_lowercase, string.punctuation + string.digits)\n",
        "  tokens = [token.translate(transTable) for token in tokens]\n",
        "  # Remove illegal tokens (check isLegalToken implementation)\n",
        "  tokens = [token for token in tokens if isLegalToken(token)]\n",
        "  return tokens\n",
        "\n",
        "def tokens_to_text(tokens : List[str]) -> str:\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "def preprocess_post(post : str) -> str:\n",
        "    tokens = clean_post(post)\n",
        "    cleanText = tokens_to_text(tokens)\n",
        "    return cleanText\n",
        "\n",
        "def preprocess_all_posts(posts : List[str]) -> List[str]:\n",
        "  return [preprocess_post(post) for post in posts]\n",
        "\n"
      ],
      "metadata": {
        "id": "zOhCz4lH0ish"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we haven't cleared our posts already, we can run:"
      ],
      "metadata": {
        "id": "BHbE7dr3G3eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processedData = {\n",
        "    'dataset' : preprocess_all_posts(dataset),\n",
        "    'labels' : labels\n",
        "}\n",
        "with open('/content/processed_reddit_data.json', 'w') as f:\n",
        "    json.dump(processedData, f)"
      ],
      "metadata": {
        "id": "rhGtD6yVbKp1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have, we can run:"
      ],
      "metadata": {
        "id": "ZzKrt76JG7o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/processed_reddit_data.json', 'r') as f:\n",
        "    processedData = json.load(f)"
      ],
      "metadata": {
        "id": "ok_eh7WDHAKq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how different the processed text is compared to our original. Let's use the second element on our training set as an example."
      ],
      "metadata": {
        "id": "wvKtU7XaF-bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processedData['dataset'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Zi4OdT0JLIju",
        "outputId": "3a2f9a0e-250e-418f-fcd0-ed67e789e453"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'much freaking tired stereotypes usually described boring robotic emotionless sure know deal dont get started memes sure arent even created thus portrayed plain people entire universe done relationship four years hope many years come dedicated downtoearth loving person ever encountered entire life usually read incompatibility pairing guess true people wouldnt trade world maybe dont know much mbti share functions right lean lean instance come different balanced relationship stormy sea lighthouse land going talking know specially hes one know extremely funny witty come punniest puns ever straight face whole room crack laughter almost like awfully bad dad jokes actually really funny emotionless robots say well somebody might think never close cant pass façade feel like everybody else closest people access emotional expression suffer love cry take time look throught serene presence get know name say loyal loyal logistician point get trapped might try take advantage might express support words extramiles demostrate actions come constant flow ideas support every one ruleobsessed thing geez sure strict personal morals principles theres key personal dont follow unconsciously everything authority figure commands throught experience adquire personal rules work doesnt mean every one principles rules considered common type mbti people mock wish healthy one wonderful people know needed move world devoted make life better loved ones thanks reading far leave comment think experience experience want based experience sorry doesnt align specifically'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "LMKVAidXLN3-",
        "outputId": "77b50f82-6226-4c21-b2ff-ce61423b7c51"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ISTJ are so much more.I\\'m so freaking tired of the ISTJ stereotypes. Usually ISTJ are described as boring, robotic, emotionless. You sure know the deal. And don\\'t get me started with the memes. I\\'m sure most of them aren\\'t even created by ISTJs themselves and thus they are portrayed as the most plain people in the entire universe. I\\'m done.\\n\\nI have been in a relationship with a ISTJ for four years now and I hope for many years to come. He is the most dedicated, down-to-earth and loving person I ever encountered in my entire life. I am myself an INFP. I usually read about the incompatibility of ISTJ and INFP pairing, and I guess it can be true to some people, but I wouldn\\'t trade my ISTJ for the world. Maybe I don\\'t know much about the MBTI but we share functions, right? I can lean on my Si, he can lean on his Fi, for instance. We can come as very different, but it\\'s a balanced relationship. I\\'m the stormy sea, he is the lighthouse on land.\\n\\nI am going to be talking about the ISTJs I know, specially my SO, as he\\'s the one that I know the most. ISTJs can be extremely funny and witty. They can come up with the punniest puns ever with a straight face and the whole room will crack of laughter. Almost like awfully bad dad jokes that are actually really funny.\\n\\nThey are emotionless robots, they say. Well, somebody that might think that have never been close to an ISTJ or can\\'t pass through the façade. Because ISTJs feel, like everybody else, but only the closest people for them can access the emotional expression. They suffer, they love, they cry. You just have to take the time to look throught their serene presence and get to know them.\\n\\nISTJs are, as their own name say, loyal. The Loyal Logistician. Once you are in, you are in. To the point that they can get trapped with some that might try to take advantage of this. They might not express their support for you with words, but they go extra-miles to demostrate you this through actions. I, as an INFP, can come up with a constant flow of ideas and my bf ISTJ will support me in each and every one of them.\\n\\nThe rule-obsessed thing. Oh, geez. Sure, ISTJs have strict personal morals and principles. But, there\\'s the key! \"Personal\". They don\\'t just follow unconsciously everything that an authority figure commands. Throught experience, they adquire some personal rules and they do work with those. That doesn\\'t mean every one that is a ISTJ has the same principles or rules.\\n\\nISTJ is considered as the most common type in the MBTI. Some people mock them for that. But I wish there were more healthy ISTJs out there. Because they are one of the most wonderful people I know. They are needed, they move the world and they are devoted to make life better for their loved ones.\\n\\nThanks for reading this far. Leave a comment about what you think about your own experience or your experience with ISTJs, if you want.\\n\\nPS: This is based on my own experience. I\\'m sorry if it doesn\\'t align with you specifically.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the Tokenizer"
      ],
      "metadata": {
        "id": "BVMLGKgY0vuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "mDtN15-Nf_Ew"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ones = ['e', 'n', 'f', 'p']\n",
        "\n",
        "personality_to_vector = {\n",
        "    'istj' : np.array([0,0,0,0]),\n",
        "    'istp' : np.array([0,0,0,1]),\n",
        "    'isfj' : np.array([0,0,1,0]),\n",
        "    'isfp' : np.array([0,0,1,1]),\n",
        "    'infj' : np.array([0,1,1,0]),\n",
        "    'infp' : np.array([0,1,1,1]),\n",
        "    'intj' : np.array([0,1,0,0]),\n",
        "    'intp' : np.array([0,1,0,1]),\n",
        "    'estp' : np.array([1,0,0,1]),\n",
        "    'estj' : np.array([1,0,0,0]),\n",
        "    'esfp' : np.array([1,0,1,1]),\n",
        "    'esfj' : np.array([1,0,1,0]),\n",
        "    'enfp' : np.array([1,1,1,1]),\n",
        "    'enfj' : np.array([1,1,1,0]),\n",
        "    'entp' : np.array([1,1,0,1]),\n",
        "    'entj' : np.array([1,1,0,0])\n",
        "}\n",
        "\n",
        "vector_code_to_personality = {\n",
        "    '0000' : 'istj',\n",
        "    '0001' : 'istp',\n",
        "    '0010' : 'isfj',\n",
        "    '0011' : 'isfp',\n",
        "    '0110' : 'infj',\n",
        "    '0111' : 'infp',\n",
        "    '0100' : 'intj',\n",
        "    '0101' : 'intp',\n",
        "    '1001' : 'estp',\n",
        "    '1000' : 'estj',\n",
        "    '1011' : 'esfp',\n",
        "    '1010' : 'esfj',\n",
        "    '1111' : 'enfp',\n",
        "    '1110' : 'enfj',\n",
        "    '1101' : 'entp',\n",
        "    '1100' : 'entj'\n",
        "}\n",
        "\n",
        "personality_ID_to_vector = {\n",
        "    0   : np.array([0,0,0,0]),\n",
        "    1   : np.array([0,0,0,1]),\n",
        "    2   : np.array([0,0,1,0]),\n",
        "    3   : np.array([0,0,1,1]),\n",
        "    4   : np.array([0,1,1,0]),\n",
        "    5   : np.array([0,1,1,1]),\n",
        "    6   : np.array([0,1,0,0]),\n",
        "    7   : np.array([0,1,0,1]),\n",
        "    8   : np.array([1,0,0,1]),\n",
        "    9   : np.array([1,0,0,0]),\n",
        "    10  : np.array([1,0,1,1]),\n",
        "    11  : np.array([1,0,1,0]),\n",
        "    12  : np.array([1,1,1,1]),\n",
        "    13  : np.array([1,1,1,0]),\n",
        "    14  : np.array([1,1,0,1]),\n",
        "    15  : np.array([1,1,0,0])\n",
        "}"
      ],
      "metadata": {
        "id": "Sgini8TJe7P0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 600\n",
        "\n",
        "tokenizer = Tokenizer(num_words = n_features)\n",
        "tokenizer.fit_on_texts(processedData['dataset'])\n",
        "\n",
        "X = tokenizer.texts_to_matrix(processedData['dataset'], mode='freq')\n",
        "Y = np.array(processedData['labels'])\n",
        "\n",
        "X, Y = shuffle(X, Y)\n",
        "\n",
        "m = X.shape[0]\n",
        "test_count = (m // 10)\n",
        "\n",
        "X_train, Y_train = X[:test_count], Y[:test_count]\n",
        "X_test, Y_test = X[test_count:], Y[test_count:]\n",
        "\n",
        "Y_train_4v = np.array([personality_ID_to_vector[val] for val in Y_train])\n",
        "Y_test_4v = np.array([personality_ID_to_vector[val] for val in Y_test])"
      ],
      "metadata": {
        "id": "RHsmgJXR0xOt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InPuqBAWJY1u",
        "outputId": "79bd308f-1410-42f3-e336-70de041928bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6814, 600)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the DNN model"
      ],
      "metadata": {
        "id": "8fNxZw-m80Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.utils import np_utils\n"
      ],
      "metadata": {
        "id": "UtZfTWhh83L_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4v = Sequential([\n",
        "    Dense(400, input_shape = (n_features, ), activation = 'relu'),\n",
        "    Dense(200, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(50, activation = 'relu'),\n",
        "    Dense(4, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_4v.compile(\n",
        "    loss = BinaryCrossentropy(),\n",
        "    optimizer = keras.optimizers.Adamax(),\n",
        "    metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "fit_history = model_4v.fit(\n",
        "    X_train,\n",
        "    Y_train_4v,\n",
        "    epochs = 50,\n",
        "    batch_size = 64,\n",
        "    validation_split = 0.1\n",
        ")"
      ],
      "metadata": {
        "id": "aKDz_EUGRrBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_16 = Sequential([\n",
        "    Dense(400, input_shape = (n_features, ), activation = 'relu'),\n",
        "    Dense(200, activation = 'relu'),\n",
        "    Dense(200, activation = 'relu'),\n",
        "    Dense(100, activation = 'relu'),\n",
        "    Dense(16, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model_16.compile(\n",
        "    loss = SparseCategoricalCrossentropy(),\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    metrics = \"accuracy\"\n",
        ")\n",
        "\n",
        "fit_history = model_16.fit(\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    epochs = 50,\n",
        "    batch_size = 64,\n",
        "    validation_split = 0.1\n",
        ")"
      ],
      "metadata": {
        "id": "dIP74Wx6mEsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def catagory_wise_accuracy_4(model, X_test, Y_test):\n",
        "  predictions = model.predict(X_test)\n",
        "  outcomes = (predictions > 0.5).astype(int)\n",
        "\n",
        "  correct = (outcomes == Y_test)\n",
        "  correct = correct.astype(int)\n",
        "\n",
        "  m = correct.shape[0]\n",
        "  correctCnt = np.sum(correct, axis = 0)\n",
        "\n",
        "  accuracy = correctCnt / m\n",
        "\n",
        "  print(accuracy)\n",
        "  "
      ],
      "metadata": {
        "id": "1AwCNUuBaPqm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def catagory_wise_accuracy_16(model, X_test, Y_test):\n",
        "  probabilities = model.predict(X_test)\n",
        "  predictions = np.argmax(probabilities, axis = 1)\n",
        "  outcomes = np.array([personality_ID_to_vector[p] for p in predictions])\n",
        "\n",
        "  correct = (outcomes == Y_test)\n",
        "  correct = correct.astype(int)\n",
        "\n",
        "  m = correct.shape[0]\n",
        "  correctCnt = np.sum(correct, axis = 0)\n",
        "\n",
        "  accuracy = correctCnt / m\n",
        "\n",
        "  print(accuracy)\n",
        "  "
      ],
      "metadata": {
        "id": "E8mSY2BmnRW5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catagory_wise_accuracy_4(model_4v, X_test, Y_test_4v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st0zP-8Emi-o",
        "outputId": "06a39989-d91d-49be-dc67-d38f3d6da017"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.56497636 0.65970977 0.53937714 0.6339475 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "catagory_wise_accuracy_16(model_16, X_test, Y_test_4v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKMXMw6fa3u6",
        "outputId": "a20b56a7-e7a4-4a89-818e-e333bb9620cc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.56807435 0.64813305 0.51443013 0.59481494]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getAccuracy_4v(X_test, Y_test):\n",
        "  predictions = model_4v.predict(X_test)\n",
        "  outcomes = (predictions > 0.5).astype(int)\n",
        "\n",
        "  correct = (outcomes == Y_test)\n",
        "  correct = correct.astype(int)\n",
        "\n",
        "  m = correct.shape[0]\n",
        "\n",
        "  correctCnt = np.sum(correct, axis = 1)\n",
        "  fullMatch = (correctCnt == 4).astype(int)\n",
        "\n",
        "  fullMatchCnt = np.sum(fullMatch, axis = 0)\n",
        "  accuracy = fullMatchCnt / m\n",
        "\n",
        "  return accuracy\n"
      ],
      "metadata": {
        "id": "h_l81C2LH9q1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getAccuracy_4v(X_test, Y_test_4v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6nDzzTmIXgq",
        "outputId": "4a8781d8-3a9b-4c5c-b91a-497c23c01b5f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13109408120006522"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_16.evaluate(X_test, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm2SIaYtJTtt",
        "outputId": "dc52c755-3214-4bac-fd74-f35578034710"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "192/192 [==============================] - 1s 4ms/step - loss: 8.8626 - accuracy: 0.1304\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8.862643241882324, 0.13044187426567078]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predict Writing"
      ],
      "metadata": {
        "id": "pgKsghClj-Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_writing_from_text(post):\n",
        "  data = preprocess_post(post)\n",
        "\n",
        "  X_predict = tokenizer.texts_to_matrix([data], mode='freq')\n",
        "  outcome = model_4v.predict(X_predict)\n",
        "\n",
        "  outcomeLabel = outcome > 0.5\n",
        "  outcomeLabel = outcomeLabel.astype(int)[0]\n",
        "  outcomeList = [str(label) for label in outcomeLabel]\n",
        "  print(outcomeList)\n",
        "  outcomeStr = ''.join(outcomeList)\n",
        "\n",
        "  personality = vector_code_to_personality[outcomeStr]\n",
        "\n",
        "  print(f'You are {personality.upper()}')\n",
        "  print(f'With the following probabilities')\n",
        "  print(outcome[0])"
      ],
      "metadata": {
        "id": "LG_12d1Pj_SA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trial writing taken from Stephen King's It.\n",
        "trial = \"Maybe there aren't any such things as good friends or bad friends - maybe there are just friends, people who stand by you when you're hurt and who help you feel not so lonely. Maybe they're always worth being scared for, and hoping for, and living for. Maybe worth dying for too, if that's what has to be. No good friends. No bad friends. Only people you want, need to be with; people who build their houses in your heart.\"\n",
        "predict_writing_from_text(trial)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dMor5sBP2sE",
        "outputId": "f85894a5-a016-4dfa-d6db-671d1dc67121"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '0', '1', '1']\n",
            "You are ESFP\n",
            "With the following probabilities\n",
            "[9.3774706e-01 1.0464571e-04 5.5766761e-01 8.2144946e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A9PbLGYIJZVm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}